 

SLO: e.g. let’s say we will respond 95% of the reqs in 2 secs # hits/sec for the users. How many users we want to process if the request comes for all the users. what should be the system
validic api: We should know the frequency of the requests from the api. Right now, it is 3 hours cron job? speed and volume of the data coming from the validic to baseline
Do we haveduplication of the messages/events in our stream and how to handle duplication?
Data reliability: For the activity whether we have user information or not. both of the activity and user queues are dependent but think of edge cases? 
Encoding of the messages: forward and backward schema compatibility
fault tolerance of the rabbitMQ ? 
events
number of partitions for the topics? right now we have 1
hyper log log to check the duplication of the messages from the queues. what if same id comes in again
kafka supports one schematics? [Are we using Exactly one delivery semantics or at least once? If at least once, how are we protecting from dups]
always save the activity data and score with timestamp what if in future they change the process of calculating the score [Look at the concept of event sourcing]
when to use kafka and spark? do we really need spark? exercise
check the frequency of polling by the kafka consumer on spark?
if the spark job is slower then there will be a lot of back pressure on the kafka queues and it will affect the hits/requests being served

https://spring.io/guides/gs/circuit-breaker/ . circuit breaker
https://activemq.apache.org/artemis/docs/1.0.0/duplicate-detection.html

incase of failure messages push it to death letter queue
death queue: http://activemq.apache.org/message-redelivery-and-dlq-handling.html
apache camel


imap protocol using javamail you can read and write both pop3 only read
(NTLM) is the authentication protocol
smbj protocol faster than smb2 

https://jcifs.samba.org/ (samba)

 
Here are comparisons between Amazon EC2 and  EFS for file storage.
 
https://kb.netgear.com/23576/What-file-sharing-protocols-can-I-use-to-access-the-shares-on-my-ReadyDATA-storage-system
 
Amazon EC2 S3 bucket vs elastic file sharing
Amazon EFS
 


Here is the analysis of the issue and also the possible resolution.
 
Story: DDEV-XXXX ( Convert email along with respective attachment (PDF to PDF) to a single file and save to an internal folder)
 
We had a Jira story to read an email from Outlook mailbox and convert to PDF and save to an internal folder. On 08/09, team requested the port details for outlook mailbox created by tech support team. Team initially had challenges accessing the mailbox as it had to be a service account enabled access, ticket was put in with tech support team (08/09) and tech support team created the service account on 08/16. Team had further issues accessing as the firewall was blocking the port 993 and requested to open up for one developers IP to test it out, ticket opened on 08/21 and got a response from tech support team on 08/24 (tech support team Response - Individual IP’s can’t be granted access to the mailbox).
 
Further diagnosis of the issue got us to the resolution. The current resolution being looked at is to open a list of subnets and security groups on AWS to access the outlook. 
 
 
 
 Major action items for the team post stand up today
 
 Access to the outlook mailbox for the dev and test environment. Currently, we already have a firewall request #44 to open imap traffic on Port #993.
Use SMB file sharing protocol on the dev environment to access the email and pdf files on the network shared folder location. Locally, files could be accessed without using SMB.
Set up PostgreSQL Database for the test environment. Right now, we have PostgreSQL Database for the dev environment.
Provide separate network folder location for the test environment like we have for the dev environment.
Moving forward we need to have sprint demos on dev environment. Features demoed locally in Sprint 1.1 didn’t work as expected on dev environment.


Right now, we have a rest controller using IMAP protocol using JavaMail to connect to the O365 with the service account details that were provided by AGT team. Next sprint we are planning to use Apache Camel route.


